# -*- coding: utf-8 -*-
"""
Twitter Analysis: MCP model (Bag-of-Words)
@author: Maxfield England

Given the twitter data generated by get_tweets.py,
learns tweet sentiment by baseline generated from VADER and
predicts additional tweets based on the model.
"""


import pickle
from numpy import array
from collections import Counter
from keras.preprocessing.text import Tokenizer
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Dense
from matplotlib import pyplot
from pandas import DataFrame
from nltk.sentiment.vader import SentimentIntensityAnalyzer

#Loading document into memory (return document text)
def load_doc(filename):
    #Open file in read-only mode
    file = open(filename, 'r')
    #read all text in file
    text = file.read()
    #close file
    file.close()
    return text

#fit a tokenizer
def create_tokenizer(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer
    
def define_model(n_words):
    #define network
    model = Sequential()
    model.add(Dense(50, input_shape=(n_words,), activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    #compile network
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    #Summarize defined model
    model.summary()
    plot_model(model, to_file='model.png', show_shapes=True)
    return model

def evaluate_mode(Xtrain, ytrain, Xtest, ytest):
    scores = list()
    n_repeats = 5
    n_words = Xtest.shape[1]
    for i in range(n_repeats):
        #define network
        model = Sequential()
        model.add(Dense(50, input_shape=(n_words,), activation='relu'))
        model.add(Dense(1, activation='sigmoid'))
        #compile network
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        #fit network
        model.fit(Xtrain, ytrain, epochs=15, verbose=2)
        #evaluate
        loss, acc = model.evaluate(Xtest, ytest, verbose=0)
        scores.append(acc)
        print("%d accuracy: %s" % ((i+1),acc))
    return scores


  
#Get training and testing corpora (tweet arrays): pre-cleaned
training = []
testing = []
trainingRaw = []
testingRaw = []
with open('training.corpus', 'rb') as trainReadFile:
    training = pickle.load(trainReadFile)
with open('testing.corpus', 'rb') as testReadFile:
    testing = pickle.load(testReadFile)
    
    
#Retrieve raw tweets (tweet arrays): non-cleaned
with open('training.raw', 'rb') as trainRawReadFile:
    trainingRaw = pickle.load(trainRawReadFile)

with open('testing.raw', 'rb') as testRawReadFile:
    testingRaw = pickle.load(testRawReadFile)    

#----------------------------------------
#Restrict vocabulary
#----------------------------------------    
#Create vocabulary using a counter for every tweet in the training corpus
vocab = Counter()

for tweet in training:
    words = tweet.split()
    vocab.update(words)


#Filter out vocab words that don't appear at least twice
min_occurrence = 2
vocab_reduced = [k for k,c in vocab.items() if c >= min_occurrence]

#Filter words not found in vocab
for tweet in training:
    words = tweet.split()
    words = [w for w in words if w in vocab_reduced]
    tweet = " ".join(words)



#--------------------------------------
# Collect VADER-assigned sentiments as our control for training values
# and use these for testing purposes (we're essentially teaching our model
# to mimic VADER here, huh?)
#--------------------------------------

#Using the raw tweets and VADER, assign positive/negative scores (ytrain/ytest)
#for learning and testing purposes
sen = SentimentIntensityAnalyzer()
ytrvals = []
for tweet in trainingRaw:
    tweetScore = sen.polarity_scores(tweet)['compound']
    #Adjust (-1, 1) score to be applicable for range (0, 1)
    tweetScore = (tweetScore+1)/2
    
    #DEBUG:
    if tweetScore >= 0.5:
        tweetScore = 1
    else:
        tweetScore = 0
    
    ytrvals.append(tweetScore)
    
ytrain = array(ytrvals)

ytevals = []
for tweet in testingRaw:
    tweetScore = sen.polarity_scores(tweet)['compound']
        #Adjust (-1, 1) score to be applicable for range (0, 1)
    tweetScore = (tweetScore+1)/2
    
    #Model only works for binary 
    if tweetScore >= 0.5:
        tweetScore = 1
    else:
        tweetScore = 0
    
    ytevals.append(tweetScore)
ytest = array(ytevals)

#-----------------------------------
# Create the model
#-----------------------------------

tokenizer = create_tokenizer(training)

Xtrain = tokenizer.texts_to_matrix(training, mode='binary')
Xtest = tokenizer.texts_to_matrix(testing, mode='binary')

#define network
n_words = Xtrain.shape[1]
model = define_model(n_words)

#fit network
model.fit(Xtrain, ytrain, epochs=15, verbose=2)

#-----------------------------------
# Evaluate the model
#-----------------------------------
#evaluate by comparing tests
loss, acc = model.evaluate(Xtest, ytest, verbose=0)
print('Test Accuracy: %f' % (acc*100))



numPos = 0
numNeg = 0
numNeutral = 0

for tweet in testing:
    encoded = tokenizer.texts_to_matrix([tweet], mode='binary')
    yhat = model.predict(encoded, verbose=0)
    percent_pos = yhat[0,0]
    if percent_pos < 0.45:
        numNeg+=1
    elif percent_pos > 0.55:
        numPos+=1
    else: 
        numNeutral+=1

print("Neural Bag-of-Words:")
print("Number of tweets that are positive:",numPos)
print("Number of tweets that are negative:",numNeg)
print("Number of tweets that are neutral:", numNeutral)


numPos = 0
numNeg = 0
numNeutral = 0

for tweet in testing:
    score = sen.polarity_scores(tweet)['compound']
    if score < -.1:
        numNeg += 1
    elif score > 0.1:
        numPos += 1
    else:
        numNeutral += 1


#Show control results (VADER)
print()
print("VADER Sentiment Analysis:")
print("Number of tweets that are positive:",numPos)
print("Number of tweets that are negative:",numNeg)
print("Number of tweets that are neutral:", numNeutral)


numPos = 0
numNeg = 0
numNeutral = 0

for tweet in testing:
    score = sen.polarity_scores(tweet)['compound']
        #Adjust (-1, 1) score to be applicable for range (0, 1)
    tweetScore = (score+1)/2
    
    if tweetScore >= 0.5:
        numPos += 1
    else:
        numNeg += 1


print()
print("VADER Polarized Sentiment Analysis:")
print("Number of tweets that are positive:",numPos)
print("Number of tweets that are negative:",numNeg)


modes = ['binary', 'count', 'tfidf', 'freq']
results = DataFrame()
for mode in modes:
    Xtrain = tokenizer.texts_to_matrix(training, mode=mode)
    Xtest = tokenizer.texts_to_matrix(testing, mode=mode)
    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)
    
print(results.describe())
results.boxplot()
pyplot.show()

